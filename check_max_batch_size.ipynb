{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from rl_training import QLearningAI\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 32 passed!\n",
      "Batch size 64 passed!\n",
      "Batch size 96 passed!\n",
      "Batch size 128 passed!\n",
      "Batch size 160 passed!\n",
      "Batch size 192 passed!\n",
      "Batch size 224 passed!\n",
      "Batch size 256 passed!\n",
      "Batch size 288 passed!\n",
      "Batch size 320 passed!\n",
      "Batch size 352 passed!\n",
      "Batch size 384 passed!\n",
      "Batch size 416 passed!\n",
      "Batch size 448 passed!\n",
      "Batch size 480 passed!\n",
      "Batch size 512 passed!\n",
      "Batch size 544 passed!\n",
      "Batch size 576 passed!\n",
      "Batch size 608 passed!\n",
      "Batch size 640 passed!\n",
      "Batch size 672 passed!\n",
      "Batch size 704 passed!\n",
      "Batch size 736 passed!\n",
      "Batch size 768 passed!\n",
      "Batch size 800 passed!\n",
      "Batch size 832 passed!\n",
      "Batch size 864 passed!\n",
      "Batch size 896 passed!\n",
      "Batch size 928 passed!\n",
      "Batch size 960 passed!\n",
      "Batch size 992 passed!\n",
      "Batch size 1024 passed!\n",
      "Maximum batch size that fits in memory: 1024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_batch_size(model, input_shape, max_batch_size=1024, step=32):\n",
    "    \"\"\"\n",
    "    Проверяет максимальный размер батча, который влезет в память.\n",
    "    \n",
    "    \n",
    "    :param model: Модель для тестирования.\n",
    "    :param input_shape: Размер входного тензора без учета размера батча.\n",
    "    :param max_batch_size: Максимальный размер батча для тестирования.\n",
    "    :param step: Шаг увеличения размера батча.\n",
    "    :return: Максимальный размер батча, который влез в память.\n",
    "    \"\"\"\n",
    "    model.eval()  # Переводим модель в режим оценки\n",
    "    with torch.no_grad():  # Отключаем вычисление градиентов\n",
    "        for batch_size in range(step, max_batch_size + step, step):\n",
    "            try:\n",
    "                # Создаем случайный входной тензор\n",
    "                input_tensor = torch.randn((batch_size, *input_shape)).to('cuda')\n",
    "                print(input_tensor.shape)\n",
    "                \n",
    "                # Прогоняем тензор через модель\n",
    "                _ = model(input_tensor)\n",
    "                \n",
    "                print(f\"Batch size {batch_size} passed!\")\n",
    "            except RuntimeError as e:\n",
    "                if 'out of memory' in str(e):\n",
    "                    print(f\"Batch size {batch_size} caused out of memory error!\")\n",
    "                    return batch_size - step\n",
    "                else:\n",
    "                    raise e\n",
    "    return max_batch_size\n",
    "\n",
    "# Пример использования:\n",
    "ai = QLearningAI(None, None)  # Инициализируйте ваш класс QLearningAI\n",
    "ai.init(-1, None, None)  # Инициализируйте модель\n",
    "max_batch = test_batch_size(ai.online_model, (17, 16, 5))\n",
    "print(f\"Maximum batch size that fits in memory: {max_batch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
